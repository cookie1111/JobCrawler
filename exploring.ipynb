{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of          Company_Name              URL  \\\n",
      "0        StudySmarter  studysmarter.de   \n",
      "1           Explosion     explosion.ai   \n",
      "2              Parloa       parloa.com   \n",
      "3            promiseQ     promiseq.com   \n",
      "4             AiSight       aisight.io   \n",
      "...               ...              ...   \n",
      "2626  moneymeets GmbH   moneymeets.com   \n",
      "2627       SABIO GmbH     getsabio.com   \n",
      "2628         Demodesk     demodesk.com   \n",
      "2629         eschbach     eschbach.com   \n",
      "2630      Onesto GmbH        onesto.de   \n",
      "\n",
      "                                                 Career  \\\n",
      "0                                                    []   \n",
      "1                                                    []   \n",
      "2                                                    []   \n",
      "3                       [https://promiseq.com/careers/]   \n",
      "4                        [https://aisight.io/en/career]   \n",
      "...                                                 ...   \n",
      "2626                     [https://moneymeets.com/jobs/]   \n",
      "2627         [https://www.getsabio.com/open-positions/]   \n",
      "2628                     [https://demodesk.com/careers]   \n",
      "2629  [https://eschbach.com/en/about-eschbach/jobs-c...   \n",
      "2630                                                 []   \n",
      "\n",
      "                                 Internal_Potential_Job  \\\n",
      "0                                                    []   \n",
      "1                                                    []   \n",
      "2                                                    []   \n",
      "3                                                    []   \n",
      "4                                                    []   \n",
      "...                                                 ...   \n",
      "2626  [mailto://jobs@moneymeets.com, https://moneyme...   \n",
      "2627                                                 []   \n",
      "2628                                                 []   \n",
      "2629  [https://eschbach.com/en/about-eschbach/jobs-c...   \n",
      "2630                                                 []   \n",
      "\n",
      "                                 External_Potential_Job  \n",
      "0                                                    []  \n",
      "1                                                    []  \n",
      "2                                                    []  \n",
      "3                                                    []  \n",
      "4                   [https://aisight.jobs.personio.de/]  \n",
      "...                                                 ...  \n",
      "2626                                                 []  \n",
      "2627  [https://serviceware-se.com/de/karriere/job-su...  \n",
      "2628  [https://de.indeed.com/cmp/Demodesk, https://w...  \n",
      "2629  [https://dhbw-loerrach.de/wirtschaftsinformati...  \n",
      "2630                                                 []  \n",
      "\n",
      "[2631 rows x 5 columns]>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DF_PATH = 'sites.pkl'\n",
    "\n",
    "df = pd.read_pickle(DF_PATH)\n",
    "levity = pd.read_pickle(\"/home/sebastjan/Documents/crawler/pages_ds/levity_ai/df.pkl\")\n",
    "df.to_csv(\"careers.csv\")\n",
    "print(df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "1044"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df.Career.astype(bool)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:  0  amount:  1\n",
      "depth:  1  amount:  48\n"
     ]
    },
    {
     "ename": "InvalidSchema",
     "evalue": "No connection adapters were found for 'mailto://security@levity.ai'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInvalidSchema\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mPageCrawler\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PageCrawler\n\u001B[1;32m      3\u001B[0m pc \u001B[38;5;241m=\u001B[39m PageCrawler({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124murl\u001B[39m\u001B[38;5;124m\"\u001B[39m:\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlevity.ai\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n\u001B[0;32m----> 4\u001B[0m a \u001B[38;5;241m=\u001B[39m \u001B[43mpc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_website_n_deep_save_html\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpages_ds/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/crawler/PageCrawler.py:151\u001B[0m, in \u001B[0;36mPageCrawler.map_website_n_deep_save_html\u001B[0;34m(self, n, max_pages, path)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdepth: \u001B[39m\u001B[38;5;124m\"\u001B[39m,i,\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m amount: \u001B[39m\u001B[38;5;124m\"\u001B[39m,\u001B[38;5;28mlen\u001B[39m(cur_lvl))\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m url \u001B[38;5;129;01min\u001B[39;00m cur_lvl:\n\u001B[0;32m--> 151\u001B[0m     cur, ext, html \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_page_urls\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexternal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_html\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m html \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    153\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/crawler/PageCrawler.py:226\u001B[0m, in \u001B[0;36mPageCrawler.get_page_urls\u001B[0;34m(self, url, external_urls, return_html)\u001B[0m\n\u001B[1;32m    224\u001B[0m domain \u001B[38;5;241m=\u001B[39m urlparse(url)\u001B[38;5;241m.\u001B[39mnetloc\n\u001B[1;32m    225\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 226\u001B[0m     re \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    227\u001B[0m     soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(re\u001B[38;5;241m.\u001B[39mcontent, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    228\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/anaconda3/envs/crawler/lib/python3.10/site-packages/requests/api.py:73\u001B[0m, in \u001B[0;36mget\u001B[0;34m(url, params, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(url, params\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \n\u001B[1;32m     65\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mget\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/crawler/lib/python3.10/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/crawler/lib/python3.10/site-packages/requests/sessions.py:587\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    582\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    583\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    584\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    585\u001B[0m }\n\u001B[1;32m    586\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 587\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    589\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/anaconda3/envs/crawler/lib/python3.10/site-packages/requests/sessions.py:695\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    692\u001B[0m hooks \u001B[38;5;241m=\u001B[39m request\u001B[38;5;241m.\u001B[39mhooks\n\u001B[1;32m    694\u001B[0m \u001B[38;5;66;03m# Get the appropriate adapter to use\u001B[39;00m\n\u001B[0;32m--> 695\u001B[0m adapter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_adapter\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[38;5;66;03m# Start time (approximately) of the request\u001B[39;00m\n\u001B[1;32m    698\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n",
      "File \u001B[0;32m~/anaconda3/envs/crawler/lib/python3.10/site-packages/requests/sessions.py:792\u001B[0m, in \u001B[0;36mSession.get_adapter\u001B[0;34m(self, url)\u001B[0m\n\u001B[1;32m    789\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m adapter\n\u001B[1;32m    791\u001B[0m \u001B[38;5;66;03m# Nothing matches :-/\u001B[39;00m\n\u001B[0;32m--> 792\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m InvalidSchema(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo connection adapters were found for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mInvalidSchema\u001B[0m: No connection adapters were found for 'mailto://security@levity.ai'"
     ]
    }
   ],
   "source": [
    "from PageCrawler import PageCrawler\n",
    "\n",
    "pc = PageCrawler({\"url\":\"levity.ai\"})\n",
    "a = pc.map_website_n_deep_save_html(path=\"pages_ds/\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To get more keywords i could use sentence embedings(Doc2Vec, sentence-transformers, universal sentence encoder)\n",
    "[try classifying based off url handle alone](https://www.researchgate.net/publication/294423145_CALA_ClAssifying_Links_Automatically_based_on_their_URL)\n",
    "i could also use doc2vec embedings for page classification\n",
    "and then one of the sentence embedings for job title classification\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}